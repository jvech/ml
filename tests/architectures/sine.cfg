[net]
loss = square ; options (square)
epochs = 1000 ; comment
alpha = 1e-4
weights_path = data/sine.bin
inputs = x
labels = y

; activation options (relu, sigmoid, softplus, leaky_relu)

[layer]
neurons=20
activation=leaky_relu

[layer]
neurons=20
activation=sigmoid

[outlayer]
activation=linear
