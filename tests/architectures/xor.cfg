[net]
loss = square ; options (square)
epochs = 1000 ; comment
alpha = 0.05
weights_path = data/xor.bin
inputs = x,y
labels = z

; activation options (relu, sigmoid, softplus, leaky_relu)

[layer]
neurons=10
activation=relu

[outlayer]
activation = sigmoid
